{
  "hash": "04b0f2aa647b24d441e703430c4fe699",
  "result": {
    "markdown": "---\ntitle: \"Bayesian Decision Analysis in PyMC\"\ndescription: Your act was unwise, I exclaimed as you see by the outcome. He solemnly eyed me. <br />\n When choosing the course of my action, said he, <br />\n I had not the outcome to guide me. - Ambrose Bierce\ndate: \"8/20/2022\"\ncategories: [Bayesian Statistics ,PyMC]\nimage: \"\"\ndraft: True\nexecute: \n  eval: false\n# highlight-style: arrow\n---\n\nWhile oftentimes Statistical Analysis stops with the hopefully successfull inference of unknown parameters of interests. , one is often interested not only in the inference of unknown parameters, but also which results these parameters imply to actions in the real world. One can see the use of a custom loss function as a way to rebalance the associated consequences. Certain outcomes can have a profound implication in the real world and the loss function gives us a way to quantify this. For example, could a misjudgement end up in financial ruin or death, or both. The decision end up being conservative even tough the posterior probabilities assign a high value to the outcome.\n\nNote that the utility function is just based on a judgement which is purely subjective. It is also not constrained to follow the principle of maximum entropy which ensures us in the choice of prior that we impose no information which we not have.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport arviz as az\nimport numpy as np\nimport pymc as pm\n```\n:::\n\n\n### 1. Space of possible Decisions and outcomes\n\nWe start by defining our set of decisions which our individual can make and the outcomes which resulted from every decision.\nIn our case we deal with a decision of a transport mode and an associated time and cost. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nmodes = [\"walk\", \"bike\", \"public\", \"cab\"]\n```\n:::\n\n\n### 2. Probability distribution of outcomes conditional on decisions\n\nEvery decision $d \\in D$ yields an outcome $x=(c, t) \\in X$. The density we need looks like the following:\n\n$$\np(x|d, x^{obs}, d^{obs})= \\int p(x|d,\\theta) \\cdot p(\\theta|x^{obs}, d^{obs})d\\theta\n$$\n\n\nThe commute time $t_n$ aswell as the cost $c_n$ are modeled as follows:\n\n\\begin{align*}\n\nt_n &\\sim lognormal(\\mu_{d[n]}, \\sigma_{d[n]})\\\\\n\\mu_k &\\sim normal(0,1) \\\\\n\\sigma_k &\\sim lognormal(0, 0.25) \\\\\n\nc_n &\\sim lognormal(\\nu_{d[n]}, \\tau_{d[n]}) \\\\\n\\nu_k &\\sim normal(0,1) \\\\\n\\tau_k &\\sim lognormal(0,0.25)\n\\end{align*}\n\n\n### 3. Defining our utility function\n\nThe utility function $U(X)$ takes an outcome and maps it to a real number which is our resulting utility. Our utility is a linear function of the cost and the time.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef utility(cost, time):\n    return -(cost+5*time)\n```\n:::\n\n\n### 4. Compute the expected utility\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nwith pm.Model(coords={\"modes\": modes}) as discrete_choice_model:\n\n    #Priors for time\n    mu = pm.Normal(\"mu\", 0, 1)\n    sigma = pm.LogNormal(\"sigma\", 0, 0.25)\n\n    time = pm.LogNormal(\"time\", mu, sigma, dims=\"modes\")\n\n    #Priors for cost\n    nu = pm.Normal(\"nu\", 0, 1)\n    tau = pm.LogNormal(\"tau\", 0, 0.25)\n\n    cost = pm.LogNormal(\"cost\", nu, tau, dims=\"modes\")\n\n    #utility \n    util = pm.Deterministic(\"utility\", utility(cost, time), dims=\"modes\")\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nwith discrete_choice_model:\n    idata = pm.sample(1000, tune=2500, random_seed=RANDOM_SEED, target_accept=0.95)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nwith discrete_choice_model:\n    pp = pm.sample_posterior_predictive(idata, random_seed=RANDOM_SEED)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\naz.summary(idata, filter_vars=\"like\", var_names=\"utility\")[\"mean\"].idxmax()\n```\n:::\n\n\nThe uncertainty in the expected utility gets propagated trough the uncertainty in the prediction, which itself gets propagated trough the uncertainty in our decision variables.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}