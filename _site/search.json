[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Blog about Bayesian Statistics",
    "section": "",
    "text": "Bayesian Statistics\n\n\nPyMC\n\n\n\n\nYour act was unwise, I exclaimed as you see by the outcome. He solemnly eyed me.  When choosing the course of my action, said he,  I had not the outcome to guide me. - Ambrose Bierce\n\n\n\n\n\n\nAug 20, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Bayesian_Decision_Analysis/index.html",
    "href": "posts/Bayesian_Decision_Analysis/index.html",
    "title": "Bayesian Decision Analysis in PyMC",
    "section": "",
    "text": "import arviz as az\nimport numpy as np\nimport pymc as pm\n\n\n1. Space of possible Decisions and outcomes\nWe start by defining our set of decisions which our individual can make and the outcomes which resulted from every decision. In our case we deal with a decision of a transport mode and an associated time and cost.\n\nmodes = [\"walk\", \"bike\", \"public\", \"cab\"]\n\n\n\n2. Probability distribution of outcomes conditional on decisions\nEvery decision \\(d \\in D\\) yields an outcome \\(x=(c, t) \\in X\\). The density we need looks like the following:\n\\[\np(x|d, x^{obs}, d^{obs})= \\int p(x|d,\\theta) \\cdot p(\\theta|x^{obs}, d^{obs})d\\theta\n\\]\nThe commute time \\(t_n\\) aswell as the cost \\(c_n\\) are modeled as follows:\n\\[\\begin{align*}\n\nt_n &\\sim lognormal(\\mu_{d[n]}, \\sigma_{d[n]})\\\\\n\\mu_k &\\sim normal(0,1) \\\\\n\\sigma_k &\\sim lognormal(0, 0.25) \\\\\n\nc_n &\\sim lognormal(\\nu_{d[n]}, \\tau_{d[n]}) \\\\\n\\nu_k &\\sim normal(0,1) \\\\\n\\tau_k &\\sim lognormal(0,0.25)\n\\end{align*}\\]\n\n\n3. Defining our utility function\nThe utility function \\(U(X)\\) takes an outcome and maps it to a real number which is our resulting utility. Our utility is a linear function of the cost and the time.\n\ndef utility(cost, time):\n    return -(cost+5*time)\n\n\n\n4. Compute the expected utility\n\nwith pm.Model(coords={\"modes\": modes}) as discrete_choice_model:\n\n    #Priors for time\n    mu = pm.Normal(\"mu\", 0, 1)\n    sigma = pm.LogNormal(\"sigma\", 0, 0.25)\n\n    time = pm.LogNormal(\"time\", mu, sigma, dims=\"modes\")\n\n    #Priors for cost\n    nu = pm.Normal(\"nu\", 0, 1)\n    tau = pm.LogNormal(\"tau\", 0, 0.25)\n\n    cost = pm.LogNormal(\"cost\", nu, tau, dims=\"modes\")\n\n    #utility \n    util = pm.Deterministic(\"utility\", utility(cost, time), dims=\"modes\")\n\n\nwith discrete_choice_model:\n    idata = pm.sample(1000, tune=2500, random_seed=RANDOM_SEED, target_accept=0.95)\n\n\nwith discrete_choice_model:\n    pp = pm.sample_posterior_predictive(idata, random_seed=RANDOM_SEED)\n\n\naz.summary(idata, filter_vars=\"like\", var_names=\"utility\")[\"mean\"].idxmax()\n\nThe uncertainty in the expected utility gets propagated trough the uncertainty in the prediction, which itself gets propagated trough the uncertainty in our decision variables."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lars Karbach",
    "section": "",
    "text": "Hey there! My name is Lars Karbach iâ€™m an enrolled Master Student in Germany. My main research interests include Bayesian Inference and Computation aswell as Machine Learning. The domains in which I prefer to use these methods are Transportation Modelling, Spatial Statistics and Finance. Feel free to follow and get in touch with me on my Socials."
  }
]